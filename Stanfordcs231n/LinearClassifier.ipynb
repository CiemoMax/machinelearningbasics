{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xiaoxiaoma/Documents/GitHub/machinelearningbasics/Stanfordcs231n\n"
     ]
    }
   ],
   "source": [
    "cd Documents/GitHub/machinelearningbasics/Stanfordcs231n/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import import_ipynb\n",
    "import SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        \n",
    "    def train(self, X, y, learning_rate = 1e-3, reg = 1e-5, \\\n",
    "              num_iters = 100, batch_size = 200, verbose = False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; \n",
    "      there are N training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; \n",
    "      y[i] = c means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - reg: (float) regularization strength.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at \n",
    "      each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training\n",
    "    iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1\n",
    "        if self.W is None:\n",
    "            # naive initialization\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "        \n",
    "        loss_history = []\n",
    "        for ite in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "            \n",
    "  #################################################################\n",
    "      # TODO:                                                         \n",
    "      # Sample batch_size elements from the training data and their \n",
    "      # corresponding labels to use in this round of gradient descent.\n",
    "      # Store the data in X_batch and their corresponding labels in \n",
    "      # y_batch; after sampling X_batch should have shape (dim, \n",
    "      # batch_size) and y_batch should have shape (batch_size,) \n",
    "        \n",
    "      # Hint: Use np.random.choice to generate indices. Sampling with \n",
    "      # replacement is faster than sampling without replacement.    \n",
    "      #############################################################\n",
    "            pass\n",
    "      #############################################################\n",
    "      #                       END OF YOUR CODE                         \n",
    "      #############################################################\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "      # perform parameter update\n",
    "      ##############################################################\n",
    "      # TODO:                                                           \n",
    "      # Update the weights using the gradient and the learning rate.    \n",
    "      ##############################################################\n",
    "            pass\n",
    "      ###############################################################\n",
    "      #                       END OF YOUR CODE                          \n",
    "      ###############################################################\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this linear classifier to predict \n",
    "    labels for data points.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; \n",
    "      there are N training samples each of dimension D.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a \n",
    "      1-dimensional array of length N, and each element is an integer \n",
    "      giving the predicted class.\n",
    "    \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "    #################################################################\n",
    "    # TODO:                                                    \n",
    "    # Implement this method. Store the predicted labels in y_pred.      \n",
    "    #################################################################\n",
    "        pass\n",
    "    #################################################################\n",
    "    #                           END OF YOUR CODE                   \n",
    "    #################################################################\n",
    "        return y_pred\n",
    "  \n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, D) containing a minibatch \n",
    "      of N data points; each point has dimension D.\n",
    "    - y_batch: A numpy array of shape (N,) containing labels for the \n",
    "      minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an array of the same shape as W\n",
    "    \"\"\"\n",
    "    pass    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return svm_loss_vectorized(self.W, X_batch, y_batch, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
