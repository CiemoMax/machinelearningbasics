{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def print_mean_std(x,axis=0):\n",
    "    print('  means: ', x.mean(axis=axis))\n",
    "    print('  stds:  ', x.std(axis=axis))\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "    print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization, a: \n",
      "  means:  [ -2.3814598  -13.18038246   1.91780462]\n",
      "  stds:   [ 27.18502186  34.21455511  37.68611762]\n",
      "\n",
      "After batch normalization (gamma=1, beta=0), a_norm:\n",
      "  means:  [  4.44089210e-17   8.27116153e-17   4.46864767e-17]\n",
      "  stds:   [ 0.99999999  1.          1.        ]\n",
      "\n",
      "After batch normalization (gamma= [ 1.  2.  3.] , beta= [ 11.  12.  13.] ), a_norm:\n",
      "  means:  [ 11.  12.  13.]\n",
      "  stds:   [ 0.99999999  1.99999999  2.99999999]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization   \n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization, a: ')\n",
    "print_mean_std(a,axis=0)\n",
    "\n",
    "gamma = np.ones((D3,))\n",
    "beta = np.zeros((D3,))\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization (gamma=1, beta=0), a_norm:')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "print('After batch normalization (gamma=', gamma, ', beta=', beta, '), a_norm:')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn_param after training:  {'mode': 'train', 'running_mean': array([ -0.32415038,  18.55718135,  14.18894184]), 'running_var': array([ 34.57738618,  35.22431942,  36.07028501])} \n",
      "\n",
      "After batch normalization (test-time), a_norm:\n",
      "  means:  [-0.23188545 -0.25906202 -0.63026885]\n",
      "  stds:   [ 5.99478954  6.03037459  5.89828084]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "\n",
    "for t in range(50):\n",
    "    X = np.random.randn(N, D1)\n",
    "    a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "    batchnorm_forward(a, gamma, beta, bn_param)\n",
    "print('bn_param after training: ', bn_param, '\\n')\n",
    "    \n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time), a_norm:')\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmu shape:  (4, 5)\n",
      "dvar shape:  (5,)\n",
      "dxmu shape:  (5,)\n",
      "dout: [[ 0.27423503  0.76215717 -0.69550058  0.29214712 -0.38489942]\n",
      " [ 0.1228747  -1.42904497  0.70286283 -0.85850947 -1.14042979]\n",
      " [-1.58535997 -0.01530138 -0.32156083  0.56834936 -0.19961722]\n",
      " [ 1.27286625  1.27292534  1.58102968 -1.75626715  0.9217743 ]] \n",
      "\n",
      "x:  [[ 14.08971705  18.98550139   3.07047846   8.45586133  11.62637342]\n",
      " [  8.12491616  11.25101049  21.30864512   4.8723535   10.1182165 ]\n",
      " [ 10.28862305  13.47453818   7.81338135  16.76093835  18.64658296]\n",
      " [ 14.62326227  11.25950008  16.44765974  12.62223264  16.95546256]]\n",
      "dx_num: [[-0.00310319  0.00305468 -0.00156246  0.17251307  0.01388029]\n",
      " [ 0.01147762 -0.10800884 -0.01112564 -0.02021632 -0.02098085]\n",
      " [-0.01682492 -0.01106847 -0.00384286  0.13581055 -0.04108612]\n",
      " [ 0.00845049  0.11602263  0.01653096 -0.2881073   0.04818669]]\n",
      "dx_ana: [[-0.00310319  0.00305468 -0.00156246  0.17251307  0.01388029]\n",
      " [ 0.01147762 -0.10800884 -0.01112564 -0.02021632 -0.02098085]\n",
      " [-0.01682492 -0.01106847 -0.00384286  0.13581055 -0.04108612]\n",
      " [ 0.00845049  0.11602263  0.01653096 -0.2881073   0.04818669]]\n",
      "dx error:  1.66746247515e-09 \n",
      "\n",
      "gamma:  [ 0.03514666  0.26207083  0.14320173  0.90101716  0.23185863]\n",
      "dg_num: [ 2.29048278  1.39248907  2.93350569  0.98234546  2.08326113]\n",
      "dg_ana: [ 2.29048278  1.39248907  2.93350569  0.98234546  2.08326113]\n",
      "dgamma error:  7.41722504069e-13 \n",
      "\n",
      "beta:  [-0.79725793  0.12001014 -0.65679608  0.26917456  0.333667  ]\n",
      "db_num: [ 0.08461601  0.59073617  1.2668311  -1.75428014 -0.80317214]\n",
      "db_ana: [[ 0.08461601  0.59073617  1.2668311  -1.75428014 -0.80317214]]\n",
      "dbeta error:  2.37944694996e-12 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda g: batchnorm_forward(x, g, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, b, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dg_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "#You should expect to see relative errors between 1e-13 and 1e-8\n",
    "\n",
    "print('dout:', dout, '\\n')\n",
    "\n",
    "print('x: ', x)\n",
    "print('dx_num:', dx_num)\n",
    "print('dx_ana:', dx)\n",
    "print('dx error: ', rel_error(dx_num, dx),'\\n')\n",
    "\n",
    "print('gamma: ', gamma)\n",
    "print('dg_num:', dg_num)\n",
    "print('dg_ana:', dgamma)\n",
    "print('dgamma error: ', rel_error(dg_num, dgamma),'\\n')\n",
    "\n",
    "print('beta: ', beta)\n",
    "print('db_num:', db_num)\n",
    "print('db_ana:', dbeta)\n",
    "print('dbeta error: ', rel_error(db_num, dbeta),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [[ 14.08971705  18.98550139   3.07047846   8.45586133  11.62637342]\n",
      " [  8.12491616  11.25101049  21.30864512   4.8723535   10.1182165 ]\n",
      " [ 10.28862305  13.47453818   7.81338135  16.76093835  18.64658296]\n",
      " [ 14.62326227  11.25950008  16.44765974  12.62223264  16.95546256]]\n",
      "dx_num: [[-0.00310319  0.00305468 -0.00156246  0.17251307  0.01388029]\n",
      " [ 0.01147762 -0.10800884 -0.01112564 -0.02021632 -0.02098085]\n",
      " [-0.01682492 -0.01106847 -0.00384286  0.13581055 -0.04108612]\n",
      " [ 0.00845049  0.11602263  0.01653096 -0.2881073   0.04818669]]\n",
      "dx_ana: [[-0.00310319  0.00305468 -0.00156246  0.17251307  0.01388029]\n",
      " [ 0.01147762 -0.10800884 -0.01112564 -0.02021632 -0.02098085]\n",
      " [-0.01682492 -0.01106847 -0.00384286  0.13581055 -0.04108612]\n",
      " [ 0.00845049  0.11602263  0.01653096 -0.2881073   0.04818669]]\n",
      "dx error:  1.66746637939e-09 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, _, _ = batchnorm_backward_alt(dout, cache)\n",
    "\n",
    "print('x: ', x)\n",
    "print('dx_num:', dx_num)\n",
    "print('dx_ana:', dx)\n",
    "print('dx error: ', rel_error(dx_num, dx),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx difference:  8.46498696262e-13\n",
      "dgamma difference:  0.0\n",
      "dbeta difference:  0.0\n",
      "speedup: 3.35x\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
    "t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
    "t3 = time.time()\n",
    "\n",
    "print('dx difference: ', rel_error(dx1, dx2))\n",
    "print('dgamma difference: ', rel_error(dgamma1, dgamma2))\n",
    "print('dbeta difference: ', rel_error(dbeta1, dbeta2))\n",
    "print('speedup: %.2fx' % ((t2 - t1) / (t3 - t2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before layer normalization:\n",
      "  means:  [-59.06673243 -47.60782686 -43.31137368 -26.40991744]\n",
      "  stds:   [ 10.07429373  28.39478981  35.28360729   4.01831507]\n",
      "\n",
      "After layer normalization (gamma=1, beta=0)\n",
      "  means:  [ -4.81096644e-16   0.00000000e+00   7.40148683e-17  -5.92118946e-16]\n",
      "  stds:   [ 0.99999995  0.99999999  1.          0.99999969]\n",
      "\n",
      "After layer normalization (gamma= [ 3.  3.  3.] , beta= [ 5.  5.  5.] )\n",
      "  means:  [ 5.  5.  5.  5.]\n",
      "  stds:   [ 2.99999985  2.99999998  2.99999999  2.99999907]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after layer normalization   \n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 =4, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before layer normalization:')\n",
    "print_mean_std(a,axis=1)\n",
    "\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After layer normalization (gamma=1, beta=0)')\n",
    "a_norm, _ = layernorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=1)\n",
    "\n",
    "gamma = np.asarray([3.0,3.0,3.0])\n",
    "beta = np.asarray([5.0,5.0,5.0])\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "print('After layer normalization (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = layernorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "%run layers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [[ 14.08971705  18.98550139   3.07047846   8.45586133  11.62637342]\n",
      " [  8.12491616  11.25101049  21.30864512   4.8723535   10.1182165 ]\n",
      " [ 10.28862305  13.47453818   7.81338135  16.76093835  18.64658296]\n",
      " [ 14.62326227  11.25950008  16.44765974  12.62223264  16.95546256]]\n",
      "dx_num: [[-0.0148552   0.01032912 -0.01190652  0.04456401 -0.02813141]\n",
      " [ 0.06974204 -0.02127583 -0.00771128 -0.04754429  0.00678935]\n",
      " [-0.01334007 -0.01950385  0.00393253  0.09003202 -0.06112062]\n",
      " [ 0.07764743  0.38964293  0.06352497 -0.56141532  0.0306    ]]\n",
      "dx_ana: [[-0.0148552   0.01032912 -0.01190652  0.04456401 -0.02813141]\n",
      " [ 0.06974204 -0.02127583 -0.00771128 -0.04754429  0.00678935]\n",
      " [-0.01334007 -0.01950385  0.00393253  0.09003202 -0.06112062]\n",
      " [ 0.07764743  0.38964293  0.06352497 -0.56141532  0.0306    ]]\n",
      "dx error:  2.10727914716e-09 \n",
      "\n",
      "gamma:  [ 0.03514666  0.26207083  0.14320173  0.90101716  0.23185863]\n",
      "dg_num: [ 1.45413018 -0.74806364  4.30445918  2.71523651  1.0074201 ]\n",
      "dg_ana: [ 1.45413018 -0.74806364  4.30445918  2.71523651  1.0074201 ]\n",
      "dgamma error:  4.51948954603e-12 \n",
      "\n",
      "beta:  [-0.79725793  0.12001014 -0.65679608  0.26917456  0.333667  ]\n",
      "db_num: [ 0.08461601  0.59073617  1.2668311  -1.75428014 -0.80317214]\n",
      "db_ana: [ 0.08461601  0.59073617  1.2668311  -1.75428014 -0.80317214]\n",
      "dbeta error:  2.58425376299e-12 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient check layernorm backward\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "ln_param = {}\n",
    "fx = lambda x: layernorm_forward(x, gamma, beta, ln_param)[0]\n",
    "fg = lambda g: layernorm_forward(x, g, beta, ln_param)[0]\n",
    "fb = lambda b: layernorm_forward(x, gamma, b, ln_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dg_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = layernorm_forward(x, gamma, beta, ln_param)\n",
    "dx, dgamma, dbeta = layernorm_backward(dout, cache)\n",
    "\n",
    "#You should expect to see relative errors between 1e-12 and 1e-8\n",
    "print('x: ', x)\n",
    "print('dx_num:', dx_num)\n",
    "print('dx_ana:', dx)\n",
    "print('dx error: ', rel_error(dx_num, dx),'\\n')\n",
    "\n",
    "print('gamma: ', gamma)\n",
    "print('dg_num:', dg_num)\n",
    "print('dg_ana:', dgamma)\n",
    "print('dgamma error: ', rel_error(dg_num, dgamma),'\\n')\n",
    "\n",
    "print('beta: ', beta)\n",
    "print('db_num:', db_num)\n",
    "print('db_ana:', dbeta)\n",
    "print('dbeta error: ', rel_error(db_num, dbeta),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
