{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from random import randrange\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the raw CIFAR-10 data.\n",
    "cifar10_dir = '/Users/xiaoxiaoma/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which \\\n",
    "# may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "\n",
    "### Subsample the data for more efficient code execution in this exercise\n",
    "mask = 0\n",
    "num_train = 500\n",
    "num_test = 100\n",
    "num_val = 100\n",
    "num_dev = 50\n",
    "\n",
    "mask = range(num_train, num_train + num_val)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "mask = range(num_train)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "mask = np.random.choice(num_train, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "\n",
    "### Reshape the image data into rows\n",
    "# print (X_train.shape[0])\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "\n",
    "### Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "# print(mean_image[:10]) # print a few of the elements\n",
    "# plt.figure(figsize=(4,4))\n",
    "# plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
    "# visualize the mean image\n",
    "# plt.show()\n",
    "\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# third: append the bias dimension of ones (i.e. bias trick) so that our \n",
    "# SVM only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(X, y, W, reg):\n",
    "\"\"\"\n",
    "  Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "  Inputs and outputs are the same as svm_loss_naive.\n",
    "\"\"\"\n",
    "    loss = 0.0\n",
    "    loss_i = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "\n",
    "# L_i=−log(e^f(y[i])/∑j e^f(j))  \n",
    "\n",
    "    for i in range(num_train):\n",
    "        summ = 0.0\n",
    "        scores = X[i].dot(W) # scores.shape -->(1,C) or should say, (C,)?\n",
    "        scores -=np.max(scores) # to imporve Numeric stability, \\\n",
    "                                # avoid potential blowup \n",
    "        for j in range(num_classes):\n",
    "            summ + = np.exp(scores[j])\n",
    "        \n",
    "        loss += -np.log(np.exp(scores[y[i]])/summ)\n",
    "    loss /= num_train\n",
    "    \n",
    "    \n",
    "######################################################################\n",
    "# TODO: Compute the softmax loss and its gradient using explicit loops.\n",
    "# Store the loss in loss and the gradient in dW. If you are not careful\n",
    "# here, it is easy to run into numeric instability. Don't forget the \n",
    "# regularization!                                                   \n",
    "#######################################################################\n",
    "    \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(X, y, W, reg):\n",
    "\"\"\"\n",
    "  Softmax loss function, vectorized version.\n",
    "\n",
    "  Inputs and outputs are the same as softmax_loss_naive.\n",
    "\"\"\"    \n",
    "    \n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "############################################################################\n",
    "# TODO: Compute the softmax loss and its gradient using no explicit loops.\n",
    "# Store the loss in loss and the gradient in dW. If you are not careful\n",
    "# here, it is easy to run into numeric instability. Don't forget the \n",
    "# regularization!                             \n",
    "#######################################################################    \n",
    "    \n",
    "    return loss, dW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
