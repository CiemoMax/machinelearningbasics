{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Documents/GitHub/machinelearningbasics/Stanfordcs231n/'\n",
      "/Users/xiaoxiaoma/Documents/GitHub/machinelearningbasics/Stanfordcs231n\n"
     ]
    }
   ],
   "source": [
    "# cd Documents/GitHub/machinelearningbasics/Stanfordcs231n/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear_Classifier.ipynb              \u001b[30m\u001b[43mcs231n\u001b[m\u001b[m/\r\n",
      "RunClassifiers.ipynb                 gradient_check.ipynb\r\n",
      "SVM.ipynb                            k_Nearest Neighbor Classifier.ipynb\r\n",
      "Softmax.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from LossF_Softmax.ipynb\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(500, 3073) (100, 3073) (100, 3073) (50, 3073)\n",
      "loss_n: 14.405674\n",
      "grad: [[  6.78324466   9.34672488  -1.98178316 -10.59933027   3.19409248]\n",
      " [  6.3844588    8.72658582  -5.9677299   -2.07035872   3.7003802 ]]\n",
      "loss_v: 14.405674\n",
      "grad: [[  6.78324466   9.34672488  -1.98178316 -10.59933027   3.19409248]\n",
      " [  6.3844588    8.72658582  -5.9677299   -2.07035872   3.7003802 ]]\n",
      "numerical: -0.344025 analytic: -0.344014, relative error: 1.529969e-05\n",
      "numerical: 2.808125 analytic: 2.808132, relative error: 1.161211e-06\n",
      "numerical: -0.804693 analytic: -0.804664, relative error: 1.840710e-05\n",
      "numerical: -0.359729 analytic: -0.359686, relative error: 5.900791e-05\n",
      "numerical: 1.861606 analytic: 1.861640, relative error: 9.114978e-06\n",
      "numerical: 0.354481 analytic: 0.354513, relative error: 4.448994e-05\n",
      "numerical: -11.578370 analytic: -11.578368, relative error: 8.746916e-08\n",
      "numerical: 0.453740 analytic: 0.453742, relative error: 2.015296e-06\n",
      "numerical: -0.856746 analytic: -0.856707, relative error: 2.240293e-05\n",
      "numerical: -1.757495 analytic: -1.757494, relative error: 1.146266e-07\n",
      "numerical: 13.465782 analytic: 13.465802, relative error: 7.761454e-07\n",
      "numerical: 1.567127 analytic: 1.567168, relative error: 1.295787e-05\n",
      "numerical: -2.499503 analytic: -2.499485, relative error: 3.556938e-06\n",
      "numerical: -10.940097 analytic: -10.940051, relative error: 2.086382e-06\n",
      "numerical: 7.898733 analytic: 7.898761, relative error: 1.800579e-06\n",
      "numerical: -0.320291 analytic: -0.320258, relative error: 5.186526e-05\n",
      "numerical: -1.692729 analytic: -1.692713, relative error: 4.900619e-06\n",
      "numerical: 0.430433 analytic: 0.430463, relative error: 3.457064e-05\n",
      "numerical: 3.278375 analytic: 3.278375, relative error: 1.165084e-07\n",
      "numerical: 3.057076 analytic: 3.057125, relative error: 8.021228e-06\n",
      "Naive loss: 1.440567e+01 computed in 0.005385s\n",
      "Vectorized loss: 1.440567e+01 computed in 0.002112s\n",
      "difference: -0.000000\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import import_ipynb\n",
    "import LossF_SVM\n",
    "import LossF_Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        \n",
    "    def train(self, X, y, learning_rate = 1e-7, reg = 1e-5, \\\n",
    "              num_iters = 1500, batch_size = 200, verbose = False):\n",
    "        \"\"\"\n",
    "        Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; \n",
    "      there are N training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; \n",
    "      y[i] = c means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - reg: (float) regularization strength.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at \n",
    "      each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training\n",
    "    iteration.\n",
    "        \"\"\"\n",
    "\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1\n",
    "        if self.W is None:\n",
    "            # naive initialization\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "#         print('original W: {}'.format(self.W[:5]))\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "    \n",
    "    #################################################################\n",
    "      # Sample batch_size elements from the training data and their \n",
    "      # corresponding labels to use in this round of gradient descent.\n",
    "      # Store the data in X_batch and their corresponding labels in \n",
    "      # y_batch; after sampling X_batch should have shape (dim, \n",
    "      # batch_size) and y_batch should have shape (batch_size,) \n",
    "        \n",
    "      # Hint: Use np.random.choice to generate indices. Sampling with \n",
    "      # replacement is faster than sampling without replacement.    \n",
    "      #############################################################\n",
    "    ##------------------- written by me start ---------------------##    \n",
    "\n",
    "            batch_indices = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "  \n",
    "    ##--------------------- written by me end ---------------------##   \n",
    "    \n",
    "    # evaluate loss and gradient\n",
    "            loss, dW = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)  \n",
    "    \n",
    "    ##############################################################\n",
    "      # Update the weights using the gradient and the learning rate.    \n",
    "      ##############################################################\n",
    "    ##------------------- written by me start ---------------------##        \n",
    "            self.W += - learning_rate * dW \n",
    "            # perform parameter update\n",
    "            \n",
    "    ##--------------------- written by me end ---------------------##          \n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "#         print('first 5 W = {}'.format(self.W[:5]))\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict \n",
    "    labels for data points.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; \n",
    "      there are N training samples each of dimension D.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a \n",
    "      1-dimensional array of length N, and each element is an integer \n",
    "      giving the predicted class.\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "    #################################################################\n",
    "    # TODO:                                                    \n",
    "    # Implement this method. Store the predicted labels in y_pred.      \n",
    "    #################################################################\n",
    "    \n",
    "    ##------------------- written by me start ---------------------##    \n",
    "#         print('first 5 W = {}'.format(self.W[:5]))\n",
    "        scores = X.dot(self.W)\n",
    "#         print('scores.shape = {}'.format(scores.shape))\n",
    "        y_pred = np.argmax(scores, axis = 1)\n",
    "    ##--------------------- written by me end ---------------------##  \n",
    "    \n",
    "        return y_pred\n",
    "  \n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, D) containing a minibatch \n",
    "      of N data points; each point has dimension D.\n",
    "    - y_batch: A numpy array of shape (N,) containing labels for the \n",
    "      minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        pass    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return svm_loss_vectorized(X_batch, y_batch, self.W, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return softmax_loss_vectorized(X_batch, y_batch, self.W, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "print('imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
