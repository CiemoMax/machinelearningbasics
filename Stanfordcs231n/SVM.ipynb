{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from random import randrange\n",
    "import time \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data.\n",
      "(500, 3073) (100, 3073) (100, 3073) (50, 3073)\n"
     ]
    }
   ],
   "source": [
    "### Load the raw CIFAR-10 data.\n",
    "cifar10_dir = '/Users/xiaoxiaoma/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which \\\n",
    "# may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "\n",
    "### Subsample the data for more efficient code execution in this exercise\n",
    "mask = 0\n",
    "num_train = 500\n",
    "num_test = 100\n",
    "num_val = 100\n",
    "num_dev = 50\n",
    "\n",
    "mask = range(num_train, num_train + num_val)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "mask = range(num_train)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "mask = np.random.choice(num_train, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "\n",
    "### Reshape the image data into rows\n",
    "# print (X_train.shape[0])\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "\n",
    "### Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "# print(mean_image[:10]) # print a few of the elements\n",
    "# plt.figure(figsize=(4,4))\n",
    "# plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
    "# visualize the mean image\n",
    "# plt.show()\n",
    "\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# third: append the bias dimension of ones (i.e. bias trick) so that our \n",
    "# SVM only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "\n",
    "# generate a random SVM weight matrix of small numbers, W\n",
    "def init_W(X, y):\n",
    "    W = np.random.rand(X.shape[1], np.max(y) + 1) * 0.0001\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "def svm_loss_naive(X, y, W, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, naive implementation (with loops).\n",
    "    unvectorized. Compute the multiclass svm loss for a single example(x,y)\n",
    "    \n",
    "    - x is a column vector representing an image (e.g. 3073 x 1 in \n",
    "    CIFAR-10)with an appended bias dimension in the 3073-rd position \n",
    "    (i.e. bias trick)\n",
    "    - y is an integer giving index of correct class (e.g. between 0 and 9)\n",
    "    - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \n",
    "    Inputs:\n",
    "\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels, (0,C); \n",
    "      y[i] = c means that X[i] has label c, where 0 <= c < C.\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    returns:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W \n",
    "    \"\"\"\n",
    "    \n",
    "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "    \n",
    "    delta = 1.0\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    \n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W) # scores.shape -->(1,C) or should say, (C,)?\n",
    "        correct_class_score = scores[y[i]]\n",
    "        num_high_margin = 0\n",
    "        \n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "                \n",
    "            margin = scores[j] - correct_class_score + delta\n",
    "#             print('margin_sample{0},class{1}: {2}'.format(i,j,margin)) #debug\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW.T[j] += X[i]\n",
    "                num_high_margin += 1\n",
    "        \n",
    "#         print('loss after loop{0}: {1}'.format(i,loss)) #debug\n",
    "        \n",
    "        dW.T[y[i]] += - num_high_margin * X[i]\n",
    "#         print('dW after loop{1}: {0}'.format(dW,i)) #debug\n",
    "  \n",
    "  # Compute the gradient (analytic gradient) of the loss function and \n",
    "  # store it dW.     \n",
    "  # it may be simpler to compute the derivative at the same time that\n",
    "  # the loss is being computed.    \n",
    "\n",
    "# Right now the loss is a sum over all training examples, but we want it\n",
    "# to be an average instead so we divide by num_train. --> 'data loss'\n",
    "# L=1/N * ∑i ∑j≠y[i] [max(0, f(x_i;W)_j - f(x_i, W)_y[j] + Δ]\n",
    "# f(x_i;W)= Wx_i\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "# Add regularization, R(W)), to the loss. R(W)=∑k∑l(W_(k,l))^2\n",
    "#     print(reg * np.sum(W*W)) #debug\n",
    "    loss += reg * np.sum(W * W)\n",
    "    dW += reg * np.sum(W * W)\n",
    "#     print('final loss: {0}'.format(loss))\n",
    "#     print('final dW: {0}'.format(dW))\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorized(X, y, W, reg):\n",
    "    \"\"\"\n",
    "    A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside \n",
    "  this function)\n",
    "\n",
    "  Inputs and outputs are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape) #dW.shape = (num_total_pixels+1, num_classes)\n",
    "    delta = 1.0\n",
    "    scores = X.dot(W) # scores.shape = (len(X[0]), num_classes)\n",
    "    correct_scores = scores[np.arange(X.shape[0]),y]\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    \n",
    "    # Implement a vectorized version of the structured SVM loss, storing \n",
    "    # the result in loss.   \n",
    "    \n",
    "    margins = np.maximum(0, ((scores.T - correct_scores).T) + delta)\n",
    "    # margins.shape = (len(X[0]), num_classes), same as scores.shape\n",
    "#     print('margins matrix before y: {0}'.format(margins))\n",
    "    margins[np.arange(X.shape[0]),y] = 0\n",
    "#     print('margins matrix: {0}'.format(margins)) #debug\n",
    "    loss = np.sum(margins)/num_train + reg * np.sum(W * W)\n",
    "#     print('loss: {0}'.format(loss)) #debug\n",
    "    margins[margins >0] = 1\n",
    "    margins[np.arange(X.shape[0]),y] = - margins.sum(axis = 1)   \n",
    "    dW = X.T.dot(margins)/num_train + reg * np.sum(W * W)\n",
    "#     print('dW: {0}'.format(dW))\n",
    "    \n",
    "    # Implement a vectorized version of the gradient for the structured \n",
    "    # SVM loss, storing the result in dW. \n",
    "\n",
    "    return loss, dW  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next cell for debug only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data.\n"
     ]
    }
   ],
   "source": [
    "################################## Debug ###############################\n",
    "\n",
    "# ### Load the raw CIFAR-10 data.\n",
    "# cifar10_dir = '/Users/xiaoxiaoma/cifar-10-batches-py'\n",
    "\n",
    "# # Cleaning up variables to prevent loading data multiple times (which \\\n",
    "# # may cause memory issue)\n",
    "# try:\n",
    "#     del X_train, y_train\n",
    "#     del X_test, y_test\n",
    "#     print('Clear previously loaded data.')\n",
    "# except:\n",
    "#     pass\n",
    "# X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# mask_trial = 1\n",
    "# X_trial = X_train[mask]\n",
    "# y_trial = y_train[mask]\n",
    "\n",
    "# X_trial = np.reshape(X_trial, (X_trial.shape[0], -1))\n",
    "# X_trial -= mean_image\n",
    "# X_trial = np.hstack([X_trial, np.ones((X_trial.shape[0], 1))])\n",
    "\n",
    "# X_trial = X_trial[:5]\n",
    "# y_trial = y_trial[:5]\n",
    "\n",
    "# W_trial = init_W(X_trial, y_trial)\n",
    "\n",
    "# loss, grad = svm_loss_naive(X_trial, y_trial, W_trial, 0.000005)\n",
    "# loss_v, grad_v = svm_loss_vectorized(X_trial, y_trial, W_trial, 0.000005)\n",
    "\n",
    "########################### Debug done ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_dev = init_W(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.937311\n",
      "(3073, 10)\n",
      "grad: [[  8.1696 -35.54    26.5696]]\n",
      "loss_v: 8.937311\n",
      "(3073, 10)\n",
      "grad_v: [[  8.1696 -35.54    26.5696]]\n"
     ]
    }
   ],
   "source": [
    "loss, grad = svm_loss_naive(X_dev, y_dev, W_dev, 0.000005)\n",
    "print('loss: %f' % (loss, ))\n",
    "print(grad.shape)\n",
    "print('grad: {0}'.format(grad[:1, :3]))\n",
    "\n",
    "loss_v, grad_v = svm_loss_vectorized(X_dev, y_dev, W_dev, 0.000005)\n",
    "print('loss_v: %f' % (loss_v, ))\n",
    "print(grad_v.shape)\n",
    "print('grad_v: {0}'.format(grad_v[:1, :3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 4.536000 analytic: 4.536000, relative error: 1.352397e-11\n",
      "numerical: 17.495200 analytic: 17.495200, relative error: 4.900638e-12\n",
      "numerical: 16.870400 analytic: 16.870400, relative error: 2.575498e-12\n",
      "numerical: 19.608400 analytic: 19.608400, relative error: 1.520399e-12\n",
      "numerical: -53.580000 analytic: -53.580000, relative error: 1.426589e-12\n",
      "numerical: 19.720800 analytic: 19.720800, relative error: 1.396707e-12\n",
      "numerical: 52.612800 analytic: 52.612800, relative error: 2.500475e-13\n",
      "numerical: -10.980000 analytic: -10.980000, relative error: 5.339505e-12\n",
      "numerical: -24.112800 analytic: -24.112800, relative error: 2.115321e-12\n",
      "numerical: 15.027600 analytic: 15.027600, relative error: 9.387944e-13\n"
     ]
    }
   ],
   "source": [
    "# Numerically compute the gradient along several randomly chosen \n",
    "# dimensions, and compare them with your analytically computed gradient. \n",
    "# The numbers should match almost exactly along all dimensions.\n",
    "\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: svm_loss_naive(X_dev, y_dev, w, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W_dev, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -7.645085 analytic: -7.645682, relative error: 3.900987e-05\n",
      "numerical: -25.999178 analytic: -26.000882, relative error: 3.277269e-05\n",
      "numerical: -55.850053 analytic: -55.849682, relative error: 3.321933e-06\n",
      "numerical: 21.337010 analytic: 21.336318, relative error: 1.621778e-05\n",
      "numerical: 29.214354 analytic: 29.213118, relative error: 2.115546e-05\n",
      "numerical: 98.709514 analytic: 98.709918, relative error: 2.050193e-06\n",
      "numerical: -11.131357 analytic: -11.134882, relative error: 1.583114e-04\n",
      "numerical: -12.157678 analytic: -12.154882, relative error: 1.149944e-04\n",
      "numerical: -21.146756 analytic: -21.151282, relative error: 1.070070e-04\n",
      "numerical: 7.694192 analytic: 7.693118, relative error: 6.974952e-05\n"
     ]
    }
   ],
   "source": [
    "# do the gradient check once again with regularization turned on\n",
    "# you didn't forget the regularization gradient did you?\n",
    "\n",
    "loss, grad = svm_loss_naive(X_dev, y_dev, W_dev, 5e1)\n",
    "f = lambda w: svm_loss_naive(X_dev, y_dev, w, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W_dev, grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 8.937311e+00 computed in 0.008822s\n",
      "Vectorized loss: 8.937311e+00 computed in 0.001195s\n",
      "difference: -0.000000\n"
     ]
    }
   ],
   "source": [
    "# Next implement the function svm_loss_vectorized; for now only compute \n",
    "# the loss; we will implement the gradient in a moment.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(X_dev, y_dev, W_dev,0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = svm_loss_vectorized(X_dev, y_dev, \\\n",
    "                                                       W_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be \\\n",
    "# much faster.\n",
    "print('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 0.011329s\n",
      "[[ 8.16960000e+00 -3.55400000e+01  2.65696000e+01 ...  9.24472000e+01\n",
      "  -4.94000000e+00 -7.88496000e+01]\n",
      " [ 2.21000000e+00 -4.08200000e+01  3.04100000e+01 ...  9.45400000e+01\n",
      "  -1.96200000e+01 -1.00450000e+02]\n",
      " [-1.15524000e+01 -2.99000000e+01  4.28476000e+01 ...  1.19903200e+02\n",
      "  -4.29000000e+01 -1.24647600e+02]\n",
      " ...\n",
      " [ 4.93520000e+00 -6.08600000e+01  1.95352000e+01 ...  2.67464000e+01\n",
      "   6.54000000e+00  2.47448000e+01]\n",
      " [ 1.48000005e-02 -6.74600000e+01  1.90148000e+01 ...  5.19736000e+01\n",
      "  -4.66000000e+00  2.58652000e+01]\n",
      " [ 6.00000001e-01  5.11827353e-10  6.00000001e-01 ... -7.99999999e-01\n",
      "   5.11827353e-10 -5.99999999e-01]]\n",
      "Vectorized loss and gradient: computed in 0.001037s\n",
      "[[ 8.16960000e+00 -3.55400000e+01  2.65696000e+01 ...  9.24472000e+01\n",
      "  -4.94000000e+00 -7.88496000e+01]\n",
      " [ 2.21000000e+00 -4.08200000e+01  3.04100000e+01 ...  9.45400000e+01\n",
      "  -1.96200000e+01 -1.00450000e+02]\n",
      " [-1.15524000e+01 -2.99000000e+01  4.28476000e+01 ...  1.19903200e+02\n",
      "  -4.29000000e+01 -1.24647600e+02]\n",
      " ...\n",
      " [ 4.93520000e+00 -6.08600000e+01  1.95352000e+01 ...  2.67464000e+01\n",
      "   6.54000000e+00  2.47448000e+01]\n",
      " [ 1.48000005e-02 -6.74600000e+01  1.90148000e+01 ...  5.19736000e+01\n",
      "  -4.66000000e+00  2.58652000e+01]\n",
      " [ 6.00000001e-01  5.11827353e-10  6.00000001e-01 ... -7.99999999e-01\n",
      "   5.11827353e-10 -5.99999999e-01]]\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of svm_loss_vectorized, and compute the \n",
    "# gradient of the loss function in a vectorized way.\n",
    "\n",
    "# The naive implementation and the vectorized implementation should match, \n",
    "# but the vectorized version should still be much faster.\n",
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(X_dev,y_dev,W_dev,0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "print(grad_naive)\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorized(X_dev, y_dev, \\\n",
    "                                         W_dev,0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "print(grad_vectorized)\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values \n",
    "# computed by the two implementations. The gradient on the other hand is \n",
    "# a matrix, so we use the Frobenius norm to compare them.\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('difference: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xiaoxiaoma/Documents/GitHub/machinelearningbasics/Stanfordcs231n\n"
     ]
    }
   ],
   "source": [
    "cd Documents/GitHub/machinelearningbasics/Stanfordcs231n/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import LinearClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of about 0.4 on the validation set.\n",
    "learning_rates = [1e-7, 5e-5]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, regularization_strength) to tuples of the form\n",
    "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
    "# of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None # The LinearSVM object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a linear SVM on the      #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the LinearSVM object that achieves this  #\n",
    "# accuracy in best_svm.                                                        #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the SVMs don't take much time to train; once you are #\n",
    "# confident that your validation code works, you should rerun the validation   #\n",
    "# code with a larger value for num_iters.                                      #\n",
    "################################################################################\n",
    "# Your code\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "import math\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best svm on test set\n",
    "y_test_pred = best_svm.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('linear SVM on raw pixels final test set accuracy: %f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class.\n",
    "# Depending on your choice of learning rate and regularization strength, these may\n",
    "# or may not be nice to look at.\n",
    "w = best_svm.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "      \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
