{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from random import randrange\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 3073) (100, 3073) (100, 3073) (50, 3073)\n"
     ]
    }
   ],
   "source": [
    "### Load the raw CIFAR-10 data.\n",
    "cifar10_dir = '/Users/xiaoxiaoma/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which \\\n",
    "# may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "\n",
    "### Subsample the data for more efficient code execution in this exercise\n",
    "mask = 0\n",
    "num_train = 500\n",
    "num_test = 100\n",
    "num_val = 100\n",
    "num_dev = 50\n",
    "\n",
    "mask = range(num_train, num_train + num_val)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "mask = range(num_train)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "mask = np.random.choice(num_train, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "\n",
    "### Reshape the image data into rows\n",
    "# print (X_train.shape[0])\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "\n",
    "### Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "# print(mean_image[:10]) # print a few of the elements\n",
    "# plt.figure(figsize=(4,4))\n",
    "# plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
    "# visualize the mean image\n",
    "# plt.show()\n",
    "\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# third: append the bias dimension of ones (i.e. bias trick) so that our \n",
    "# SVM only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "\n",
    "# generate a random SVM weight matrix of small numbers, W, shape(num_pixels + 1, num_classes)\n",
    "def init_W(X, y):\n",
    "    W = np.random.rand(X.shape[1], np.max(y) + 1) * 0.01\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(X, y, W, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "  Inputs and outputs are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)  #dW.shape (num_pixels+1, num_classes)\n",
    "######################################################################\n",
    "# Compute the softmax loss and its gradient using explicit loops.\n",
    "# Store the loss in loss and the gradient in dW. If you are not careful\n",
    "# here, it is easy to run into numeric instability. Don't forget the \n",
    "# regularization!                                                   \n",
    "#######################################################################    \n",
    "##--------------------- written by me start ---------------------##    \n",
    "    num_classes = W.shape[1]\n",
    "    num_samples = X.shape[0]\n",
    "\n",
    "# L_i=−log(e^(f(y[i])+logC)/∑j e^(f(j)+logC)  \n",
    "    for i in range(num_samples):\n",
    "        probs = []\n",
    "        scores = X[i].dot(W) # scores.shape -->(1,C) or should say, (C,)?\n",
    "        scores -= np.max(scores) # to imporve Numeric stability/avoid potential blowup\n",
    "        probs = np.exp(scores)/ np.sum(np.exp(scores))\n",
    "        correct_prob = probs[y[i]]       \n",
    "        loss += - np.log(correct_prob)\n",
    "        \n",
    "        probs[y[i]] -= 1\n",
    "        dW += np.dot(X[i].reshape(len(X[i]),1), probs.reshape(1, len(probs)))\n",
    "        \n",
    "    loss /= num_samples\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "    \n",
    "    dW /= num_samples\n",
    "    dW += reg * W\n",
    "\n",
    "##--------------------- written by me end -----------------------##    \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(X, y, W, reg):\n",
    "    \"\"\"\n",
    "    Softmax loss function, vectorized version.\n",
    "\n",
    "  Inputs and outputs are the same as softmax_loss_naive.\n",
    "    \"\"\"  \n",
    "    \n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "############################################################################\n",
    "# Compute the softmax loss and its gradient using no explicit loops.\n",
    "# Store the loss in loss and the gradient in dW. If you are not careful\n",
    "# here, it is easy to run into numeric instability. Don't forget the \n",
    "# regularization!                             \n",
    "####################################################################### \n",
    "##--------------------- written by me start ---------------------##    \n",
    "\n",
    "    num_samples = X.shape[0]\n",
    "    probs = [] # probs.shape (num_samples, num_classes)\n",
    "    \n",
    "    # Loss = -log(probability of y_i) , in which:\n",
    "    # probability of k = exp(f_k)/ (∑j exp(f_j))\n",
    "    scores = np.dot(X, W)    # scores.shape (num_samples, num_classes)\n",
    "    scores -= np.max(scores, axis = 1, keepdims = True)\n",
    "    probs = np.exp(scores) / np.sum(np.exp(scores), axis = 1, keepdims = True) \n",
    "    # probs.shape (num_samples, num_classes)\n",
    "    correct_logprobs = -np.log(probs[np.arange(X.shape[0]), y]) \n",
    "    loss = np.sum(correct_logprobs) / num_samples\n",
    "    loss += 0.5 * reg * np.sum(W*W)\n",
    "    \n",
    "    # gradient = probability_j - (1 for j == y_i, 0 for others) \n",
    "    probs [np.arange(X.shape[0]),y] -= 1\n",
    "    \n",
    "    dW = np.dot(X.T, probs)/num_samples\n",
    "    dW += reg * W\n",
    "\n",
    "##--------------------- written by me end -----------------------##      \n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_dev = init_W(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_n: 14.766565\n",
      "grad: [[ 0.86271805  8.68898425  0.0693074   6.5781696  -1.07013451]\n",
      " [-0.60935599 10.09936176  0.25420206 11.37650273 -0.57086089]]\n",
      "loss_v: 14.766565\n",
      "grad: [[ 0.86271805  8.68898425  0.0693074   6.5781696  -1.07013451]\n",
      " [-0.60935599 10.09936176  0.25420206 11.37650273 -0.57086089]]\n"
     ]
    }
   ],
   "source": [
    "loss_n, grad_n = softmax_loss_naive(X_dev, y_dev, W_dev, 0.005)\n",
    "print('loss_n: %f' % (loss_n, ))\n",
    "print('grad: {0}'.format(grad_n[:2,:5]))\n",
    "loss_v, grad_v = softmax_loss_vectorized(X_dev, y_dev, W_dev, 0.005)\n",
    "print('loss_v: %f' % (loss_v, ))\n",
    "print('grad: {0}'.format(grad_v[:2,:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs231n.gradient_check import grad_check_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.468690 analytic: 2.468723, relative error: 6.692024e-06\n",
      "numerical: 5.427471 analytic: 5.427519, relative error: 4.482002e-06\n",
      "numerical: 0.263651 analytic: 0.263666, relative error: 2.699816e-05\n",
      "numerical: 19.652035 analytic: 19.652066, relative error: 7.733647e-07\n",
      "numerical: 12.697539 analytic: 12.697562, relative error: 8.959169e-07\n",
      "numerical: -6.090680 analytic: -6.090634, relative error: 3.821983e-06\n",
      "numerical: -5.842322 analytic: -5.842304, relative error: 1.525617e-06\n",
      "numerical: 2.152431 analytic: 2.152445, relative error: 3.130806e-06\n",
      "numerical: -4.575062 analytic: -4.575018, relative error: 4.771096e-06\n",
      "numerical: -5.614247 analytic: -5.614236, relative error: 1.015407e-06\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: softmax_loss_naive(X_dev, y_dev, w, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W_dev, grad_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.617982 analytic: 0.618001, relative error: 1.483394e-05\n",
      "numerical: 2.382215 analytic: 2.382227, relative error: 2.496507e-06\n",
      "numerical: 5.023994 analytic: 5.023996, relative error: 2.260507e-07\n",
      "numerical: 6.675799 analytic: 6.675847, relative error: 3.604362e-06\n",
      "numerical: 5.071312 analytic: 5.071315, relative error: 2.112259e-07\n",
      "numerical: -5.181655 analytic: -5.181619, relative error: 3.455626e-06\n",
      "numerical: 10.467620 analytic: 10.467667, relative error: 2.262698e-06\n",
      "numerical: -3.301477 analytic: -3.301446, relative error: 4.689652e-06\n",
      "numerical: -1.136568 analytic: -1.136533, relative error: 1.535484e-05\n",
      "numerical: -2.178326 analytic: -2.178322, relative error: 7.574215e-07\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: softmax_loss_vectorized(X_dev, y_dev, w, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W_dev, grad_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 1.476657e+01 computed in 0.010930s\n",
      "Vectorized loss: 1.476657e+01 computed in 0.002462s\n",
      "difference: -0.000000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(X_dev, y_dev, W_dev,0.005)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(X_dev, y_dev, \\\n",
    "                                                       W_dev, 0.005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be \\\n",
    "# much faster.\n",
    "print('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
