{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from random import randrange\n",
    "import time \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data.\n",
      "(500, 3073) (100, 3073) (100, 3073) (50, 3073)\n"
     ]
    }
   ],
   "source": [
    "### Load the raw CIFAR-10 data.\n",
    "cifar10_dir = '/Users/xiaoxiaoma/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which \\\n",
    "# may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "\n",
    "### Subsample the data for more efficient code execution in this exercise\n",
    "mask = 0\n",
    "num_train = 500\n",
    "num_test = 100\n",
    "num_val = 100\n",
    "num_dev = 50\n",
    "\n",
    "mask = range(num_train, num_train + num_val)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "mask = range(num_train)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "mask = np.random.choice(num_train, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "\n",
    "### Reshape the image data into rows\n",
    "# print (X_train.shape[0])\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "\n",
    "### Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "# print(mean_image[:10]) # print a few of the elements\n",
    "# plt.figure(figsize=(4,4))\n",
    "# plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) \n",
    "# visualize the mean image\n",
    "# plt.show()\n",
    "\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n",
    "# third: append the bias dimension of ones (i.e. bias trick) so that our \n",
    "# SVM only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "\n",
    "# generate a random SVM weight matrix of small numbers, W\n",
    "def init_W(X, y):\n",
    "    W = np.random.rand(X.shape[1], np.max(y) + 1) * 0.0001\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "def svm_loss_naive(X, y, W, reg):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, naive implementation (with loops).\n",
    "    unvectorized. Compute the multiclass svm loss for a single example(x,y)\n",
    "    \n",
    "    - x is a column vector representing an image (e.g. 3073 x 1 in \n",
    "    CIFAR-10)with an appended bias dimension in the 3073-rd position \n",
    "    (i.e. bias trick)\n",
    "    - y is an integer giving index of correct class (e.g. between 0 and 9)\n",
    "    - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \n",
    "    Inputs:\n",
    "\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels, (0,C); \n",
    "      y[i] = c means that X[i] has label c, where 0 <= c < C.\n",
    "    - W: A numpy array of shape (D, C) containing weights.\n",
    "    - reg: (float) regularization strength\n",
    "\n",
    "    returns:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W \n",
    "    \"\"\"\n",
    "    \n",
    "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "    \n",
    "    delta = 1.0\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    \n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W) # scores.shape -->(1,C) or should say, (C,)?\n",
    "        correct_class_score = scores[y[i]]\n",
    "        num_high_margin = 0\n",
    "        \n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "                \n",
    "            margin = scores[j] - correct_class_score + delta\n",
    "#             print('margin_sample{0},class{1}: {2}'.format(i,j,margin)) #debug\n",
    "            if margin > 0:\n",
    "                loss += margin\n",
    "                dW.T[j] += X[i]\n",
    "                num_high_margin += 1\n",
    "        \n",
    "#         print('loss after loop{0}: {1}'.format(i,loss)) #debug\n",
    "        \n",
    "        dW.T[y[i]] += - num_high_margin * X[i]\n",
    "#         print('dW after loop{1}: {0}'.format(dW,i)) #debug\n",
    "  \n",
    "  # Compute the gradient (analytic gradient) of the loss function and \n",
    "  # store it dW.     \n",
    "  # it may be simpler to compute the derivative at the same time that\n",
    "  # the loss is being computed.    \n",
    "\n",
    "# Right now the loss is a sum over all training examples, but we want it\n",
    "# to be an average instead so we divide by num_train. --> 'data loss'\n",
    "# L=1/N * ∑i ∑j≠y[i] [max(0, f(x_i;W)_j - f(x_i, W)_y[j] + Δ]\n",
    "# f(x_i;W)= Wx_i\n",
    "    loss /= num_train\n",
    "    dW /= num_train\n",
    "\n",
    "# Add regularization, R(W)), to the loss. R(W)=∑k∑l(W_(k,l))^2\n",
    "#     print(reg * np.sum(W*W)) #debug\n",
    "    loss += reg * np.sum(W * W)\n",
    "    dW += reg * np.sum(W * W)\n",
    "#     print('final loss: {0}'.format(loss))\n",
    "#     print('final dW: {0}'.format(dW))\n",
    "\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorized(X, y, W, reg):\n",
    "    \"\"\"\n",
    "    A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside \n",
    "  this function)\n",
    "\n",
    "  Inputs and outputs are the same as svm_loss_naive.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dW = np.zeros(W.shape) #dW.shape = (num_total_pixels+1, num_classes)\n",
    "    delta = 1.0\n",
    "    scores = X.dot(W) # scores.shape = (len(X[0]), num_classes)\n",
    "    correct_scores = scores[np.arange(X.shape[0]),y]\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    \n",
    "    # Implement a vectorized version of the structured SVM loss, storing \n",
    "    # the result in loss.   \n",
    "    \n",
    "    margins = np.maximum(0, ((scores.T - correct_scores).T) + delta)\n",
    "    # margins.shape = (len(X[0]), num_classes), same as scores.shape\n",
    "#     print('margins matrix before y: {0}'.format(margins))\n",
    "    margins[np.arange(X.shape[0]),y] = 0\n",
    "#     print('margins matrix: {0}'.format(margins)) #debug\n",
    "    loss = np.sum(margins)/num_train + reg * np.sum(W * W)\n",
    "#     print('loss: {0}'.format(loss)) #debug\n",
    "    margins[margins >0] = 1\n",
    "    margins[np.arange(X.shape[0]),y] = - margins.sum(axis = 1)   \n",
    "    dW = X.T.dot(margins)/num_train + reg * np.sum(W * W)\n",
    "#     print('dW: {0}'.format(dW))\n",
    "    \n",
    "    # Implement a vectorized version of the gradient for the structured \n",
    "    # SVM loss, storing the result in dW. \n",
    "\n",
    "    return loss, dW  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear previously loaded data.\n"
     ]
    }
   ],
   "source": [
    "################################## Debug ###############################\n",
    "### Load the raw CIFAR-10 data.\n",
    "cifar10_dir = '/Users/xiaoxiaoma/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which \\\n",
    "# may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "mask_trial = 1\n",
    "X_trial = X_train[mask]\n",
    "y_trial = y_train[mask]\n",
    "\n",
    "X_trial = np.reshape(X_trial, (X_trial.shape[0], -1))\n",
    "X_trial -= mean_image\n",
    "X_trial = np.hstack([X_trial, np.ones((X_trial.shape[0], 1))])\n",
    "\n",
    "X_trial = X_trial[:5]\n",
    "y_trial = y_trial[:5]\n",
    "\n",
    "W_trial = init_W(X_trial, y_trial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "margin_sample0,class0: 1.0810749439607918\n",
      "margin_sample0,class1: 1.0322220165202713\n",
      "margin_sample0,class2: 1.1617050123836488\n",
      "margin_sample0,class3: 0.8748982298823402\n",
      "margin_sample0,class4: 0.9540820388945699\n",
      "margin_sample0,class5: 1.0173690185622375\n",
      "margin_sample0,class7: 1.0228890889583369\n",
      "margin_sample0,class8: 1.2093144526470074\n",
      "margin_sample0,class9: 1.1032131176981852\n",
      "loss after loop0: 9.456767919507389\n",
      "dW after loop0: [[-74.484 -74.484 -74.484 ... -74.484 -74.484 -74.484]\n",
      " [-75.95  -75.95  -75.95  ... -75.95  -75.95  -75.95 ]\n",
      " [-71.754 -71.754 -71.754 ... -71.754 -71.754 -71.754]\n",
      " ...\n",
      " [-34.008 -34.008 -34.008 ... -34.008 -34.008 -34.008]\n",
      " [-41.542 -41.542 -41.542 ... -41.542 -41.542 -41.542]\n",
      " [  1.      1.      1.    ...   1.      1.      1.   ]]\n",
      "margin_sample1,class0: 1.0730612875620475\n",
      "margin_sample1,class1: 0.8691195422642584\n",
      "margin_sample1,class2: 0.856198882358358\n",
      "margin_sample1,class3: 0.8940638791777222\n",
      "margin_sample1,class4: 1.0088170566291266\n",
      "margin_sample1,class5: 0.8396779131677403\n",
      "margin_sample1,class6: 0.9574424139928683\n",
      "margin_sample1,class7: 0.7777976152704507\n",
      "margin_sample1,class8: 0.94605494519469\n",
      "loss after loop1: 17.67900145512465\n",
      "dW after loop1: [[ -53.968  -53.968  -53.968 ...  -53.968  -53.968 -259.128]\n",
      " [ -36.9    -36.9    -36.9   ...  -36.9    -36.9   -427.4  ]\n",
      " [ -19.508  -19.508  -19.508 ...  -19.508  -19.508 -541.968]\n",
      " ...\n",
      " [ -27.016  -27.016  -27.016 ...  -27.016  -27.016  -96.936]\n",
      " [ -11.084  -11.084  -11.084 ...  -11.084  -11.084 -315.664]\n",
      " [   2.       2.       2.    ...    2.       2.      -8.   ]]\n",
      "margin_sample2,class0: 1.0686703748776356\n",
      "margin_sample2,class1: 1.1830894536797127\n",
      "margin_sample2,class2: 1.1239394280217327\n",
      "margin_sample2,class3: 1.307083866766058\n",
      "margin_sample2,class4: 1.2637267715390879\n",
      "margin_sample2,class5: 0.9699665158930761\n",
      "margin_sample2,class6: 1.092097865691469\n",
      "margin_sample2,class7: 1.3807927960839952\n",
      "margin_sample2,class8: 1.162094675966038\n",
      "loss after loop2: 28.23046320364345\n",
      "dW after loop2: [[   67.548    67.548    67.548 ...    67.548    67.548 -1352.772]\n",
      " [   80.15     80.15     80.15  ...    80.15     80.15  -1480.85 ]\n",
      " [  100.738   100.738   100.738 ...   100.738   100.738 -1624.182]\n",
      " ...\n",
      " [  -67.024   -67.024   -67.024 ...   -67.024   -67.024   263.136]\n",
      " [  -40.626   -40.626   -40.626 ...   -40.626   -40.626   -49.786]\n",
      " [    3.        3.        3.    ...     3.        3.      -17.   ]]\n",
      "margin_sample3,class0: 1.0319942608002624\n",
      "margin_sample3,class1: 0.9613589071987461\n",
      "margin_sample3,class2: 1.0629786936238297\n",
      "margin_sample3,class3: 0.8718525029944955\n",
      "margin_sample3,class5: 0.9911282275859916\n",
      "margin_sample3,class6: 0.9563072315784584\n",
      "margin_sample3,class7: 0.7961657773605326\n",
      "margin_sample3,class8: 1.2664320315770956\n",
      "margin_sample3,class9: 1.2006555604810467\n",
      "loss after loop3: 37.369336396843906\n",
      "dW after loop3: [[  -37.936   -37.936   -37.936 ...   -37.936   -37.936 -1458.256]\n",
      " [  -32.8     -32.8     -32.8   ...   -32.8     -32.8   -1593.8  ]\n",
      " [  -24.016   -24.016   -24.016 ...   -24.016   -24.016 -1748.936]\n",
      " ...\n",
      " [ -128.032  -128.032  -128.032 ...  -128.032  -128.032   202.128]\n",
      " [ -108.168  -108.168  -108.168 ...  -108.168  -108.168  -117.328]\n",
      " [    4.        4.        4.    ...     4.        4.      -16.   ]]\n",
      "margin_sample4,class0: 1.0596401646129334\n",
      "margin_sample4,class2: 1.0319284355349159\n",
      "margin_sample4,class3: 0.977206084321355\n",
      "margin_sample4,class4: 0.9710430651720596\n",
      "margin_sample4,class5: 0.9516959198967805\n",
      "margin_sample4,class6: 1.1529934662008574\n",
      "margin_sample4,class7: 0.9624547827531607\n",
      "margin_sample4,class8: 1.0929807625071826\n",
      "margin_sample4,class9: 1.0248956386191619\n",
      "loss after loop4: 46.59417471646231\n",
      "dW after loop4: [[-1.42000e+00 -3.66580e+02 -1.42000e+00 ... -1.42000e+00 -1.42000e+00\n",
      "  -1.42174e+03]\n",
      " [ 9.25000e+00 -4.11250e+02  9.25000e+00 ...  9.25000e+00  9.25000e+00\n",
      "  -1.55175e+03]\n",
      " [ 3.92300e+01 -5.93230e+02  3.92300e+01 ...  3.92300e+01  3.92300e+01\n",
      "  -1.68569e+03]\n",
      " ...\n",
      " [-1.77040e+02  3.13040e+02 -1.77040e+02 ... -1.77040e+02 -1.77040e+02\n",
      "   1.53120e+02]\n",
      " [-1.41710e+02  1.93710e+02 -1.41710e+02 ... -1.41710e+02 -1.41710e+02\n",
      "  -1.50870e+02]\n",
      " [ 5.00000e+00 -5.00000e+00  5.00000e+00 ...  5.00000e+00  5.00000e+00\n",
      "  -1.50000e+01]]\n",
      "final loss: 9.318834943803372\n",
      "final dW: [[-2.83999999e-01 -7.33160000e+01 -2.83999999e-01 ... -2.83999999e-01\n",
      "  -2.83999999e-01 -2.84348000e+02]\n",
      " [ 1.85000000e+00 -8.22500000e+01  1.85000000e+00 ...  1.85000000e+00\n",
      "   1.85000000e+00 -3.10350000e+02]\n",
      " [ 7.84600000e+00 -1.18646000e+02  7.84600000e+00 ...  7.84600000e+00\n",
      "   7.84600000e+00 -3.37138000e+02]\n",
      " ...\n",
      " [-3.54080000e+01  6.26080000e+01 -3.54080000e+01 ... -3.54080000e+01\n",
      "  -3.54080000e+01  3.06240000e+01]\n",
      " [-2.83420000e+01  3.87420000e+01 -2.83420000e+01 ... -2.83420000e+01\n",
      "  -2.83420000e+01 -3.01740000e+01]\n",
      " [ 1.00000000e+00 -9.99999999e-01  1.00000000e+00 ...  1.00000000e+00\n",
      "   1.00000000e+00 -3.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "######################### debug ################################\n",
    "# loss, grad = svm_loss_naive(X_trial, y_trial, W_trial, 0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "margins matrix: [[1.08107494 1.03222202 1.16170501 0.87489823 0.95408204 1.01736902\n",
      "  0.         1.02288909 1.20931445 1.10321312]\n",
      " [1.07306129 0.86911954 0.85619888 0.89406388 1.00881706 0.83967791\n",
      "  0.95744241 0.77779762 0.94605495 0.        ]\n",
      " [1.06867037 1.18308945 1.12393943 1.30708387 1.26372677 0.96996652\n",
      "  1.09209787 1.3807928  1.16209468 0.        ]\n",
      " [1.03199426 0.96135891 1.06297869 0.8718525  0.         0.99112823\n",
      "  0.95630723 0.79616578 1.26643203 1.20065556]\n",
      " [1.05964016 0.         1.03192844 0.97720608 0.97104307 0.95169592\n",
      "  1.15299347 0.96245478 1.09298076 1.02489564]]\n",
      "loss: 9.318834943803367\n",
      "dW: [[-2.83999999e-01 -7.33160000e+01 -2.83999999e-01 ... -2.83999999e-01\n",
      "  -2.83999999e-01 -2.84348000e+02]\n",
      " [ 1.85000000e+00 -8.22500000e+01  1.85000000e+00 ...  1.85000000e+00\n",
      "   1.85000000e+00 -3.10350000e+02]\n",
      " [ 7.84600000e+00 -1.18646000e+02  7.84600000e+00 ...  7.84600000e+00\n",
      "   7.84600000e+00 -3.37138000e+02]\n",
      " ...\n",
      " [-3.54080000e+01  6.26080000e+01 -3.54080000e+01 ... -3.54080000e+01\n",
      "  -3.54080000e+01  3.06240000e+01]\n",
      " [-2.83420000e+01  3.87420000e+01 -2.83420000e+01 ... -2.83420000e+01\n",
      "  -2.83420000e+01 -3.01740000e+01]\n",
      " [ 1.00000000e+00 -9.99999999e-01  1.00000000e+00 ...  1.00000000e+00\n",
      "   1.00000000e+00 -3.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "######################### debug ################################\n",
    "# loss_v, grad_v = svm_loss_vectorized(X_trial, y_trial, W_trial, 0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_dev = init_W(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.937311\n",
      "(3073, 10)\n",
      "grad: [[  8.1696 -35.54    26.5696]]\n",
      "loss_v: 8.937311\n",
      "(3073, 10)\n",
      "grad_v: [[  8.1696 -35.54    26.5696]]\n"
     ]
    }
   ],
   "source": [
    "loss, grad = svm_loss_naive(X_dev, y_dev, W_dev, 0.000005)\n",
    "print('loss: %f' % (loss, ))\n",
    "print(grad.shape)\n",
    "print('grad: {0}'.format(grad[:1, :3]))\n",
    "\n",
    "loss_v, grad_v = svm_loss_vectorized(X_dev, y_dev, W_dev, 0.000005)\n",
    "print('loss_v: %f' % (loss_v, ))\n",
    "print(grad_v.shape)\n",
    "print('grad_v: {0}'.format(grad_v[:1, :3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.937311\n",
      "grad: [[ 8.169600e+00 -3.554000e+01  2.656960e+01 ...  9.244720e+01\n",
      "  -4.940000e+00 -7.884960e+01]\n",
      " [ 2.210000e+00 -4.082000e+01  3.041000e+01 ...  9.454000e+01\n",
      "  -1.962000e+01 -1.004500e+02]\n",
      " [-1.155240e+01 -2.990000e+01  4.284760e+01 ...  1.199032e+02\n",
      "  -4.290000e+01 -1.246476e+02]\n",
      " ...\n",
      " [ 4.935200e+00 -6.086000e+01  1.953520e+01 ...  2.674640e+01\n",
      "   6.540000e+00  2.474480e+01]\n",
      " [ 1.480000e-02 -6.746000e+01  1.901480e+01 ...  5.197360e+01\n",
      "  -4.660000e+00  2.586520e+01]\n",
      " [ 6.000000e-01  0.000000e+00  6.000000e-01 ... -8.000000e-01\n",
      "   0.000000e+00 -6.000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Once you've implemented the gradient, recompute it with the code below\n",
    "# and gradient check it with the function we provided for you\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "\n",
    "loss, grad = svm_loss_naive(X_dev, y_dev, W_dev,0.0)\n",
    "print('loss: %f' % (loss, ))\n",
    "print('grad: {0}'.format(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 4.536000 analytic: 4.536000, relative error: 1.352397e-11\n",
      "numerical: 17.495200 analytic: 17.495200, relative error: 4.900638e-12\n",
      "numerical: 16.870400 analytic: 16.870400, relative error: 2.575498e-12\n",
      "numerical: 19.608400 analytic: 19.608400, relative error: 1.520399e-12\n",
      "numerical: -53.580000 analytic: -53.580000, relative error: 1.426589e-12\n",
      "numerical: 19.720800 analytic: 19.720800, relative error: 1.396707e-12\n",
      "numerical: 52.612800 analytic: 52.612800, relative error: 2.500475e-13\n",
      "numerical: -10.980000 analytic: -10.980000, relative error: 5.339505e-12\n",
      "numerical: -24.112800 analytic: -24.112800, relative error: 2.115321e-12\n",
      "numerical: 15.027600 analytic: 15.027600, relative error: 9.387944e-13\n"
     ]
    }
   ],
   "source": [
    "# Numerically compute the gradient along several randomly chosen \n",
    "# dimensions, and compare them with your analytically computed gradient. \n",
    "# The numbers should match almost exactly along all dimensions.\n",
    "\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: svm_loss_naive(X_dev, y_dev, w, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W_dev, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -7.645085 analytic: -7.645682, relative error: 3.900987e-05\n",
      "numerical: -25.999178 analytic: -26.000882, relative error: 3.277269e-05\n",
      "numerical: -55.850053 analytic: -55.849682, relative error: 3.321933e-06\n",
      "numerical: 21.337010 analytic: 21.336318, relative error: 1.621778e-05\n",
      "numerical: 29.214354 analytic: 29.213118, relative error: 2.115546e-05\n",
      "numerical: 98.709514 analytic: 98.709918, relative error: 2.050193e-06\n",
      "numerical: -11.131357 analytic: -11.134882, relative error: 1.583114e-04\n",
      "numerical: -12.157678 analytic: -12.154882, relative error: 1.149944e-04\n",
      "numerical: -21.146756 analytic: -21.151282, relative error: 1.070070e-04\n",
      "numerical: 7.694192 analytic: 7.693118, relative error: 6.974952e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# do the gradient check once again with regularization turned on\n",
    "# you didn't forget the regularization gradient did you?\n",
    "\n",
    "loss, grad = svm_loss_naive(X_dev, y_dev, W_dev, 5e1)\n",
    "f = lambda w: svm_loss_naive(X_dev, y_dev, w, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W_dev, grad)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 8.937311e+00 computed in 0.008822s\n",
      "Vectorized loss: 8.937311e+00 computed in 0.001195s\n",
      "difference: -0.000000\n"
     ]
    }
   ],
   "source": [
    "# Next implement the function svm_loss_vectorized; for now only compute \n",
    "# the loss; we will implement the gradient in a moment.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(X_dev, y_dev, W_dev,0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = svm_loss_vectorized(X_dev, y_dev, \\\n",
    "                                                       W_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be \\\n",
    "# much faster.\n",
    "print('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 0.011329s\n",
      "[[ 8.16960000e+00 -3.55400000e+01  2.65696000e+01 ...  9.24472000e+01\n",
      "  -4.94000000e+00 -7.88496000e+01]\n",
      " [ 2.21000000e+00 -4.08200000e+01  3.04100000e+01 ...  9.45400000e+01\n",
      "  -1.96200000e+01 -1.00450000e+02]\n",
      " [-1.15524000e+01 -2.99000000e+01  4.28476000e+01 ...  1.19903200e+02\n",
      "  -4.29000000e+01 -1.24647600e+02]\n",
      " ...\n",
      " [ 4.93520000e+00 -6.08600000e+01  1.95352000e+01 ...  2.67464000e+01\n",
      "   6.54000000e+00  2.47448000e+01]\n",
      " [ 1.48000005e-02 -6.74600000e+01  1.90148000e+01 ...  5.19736000e+01\n",
      "  -4.66000000e+00  2.58652000e+01]\n",
      " [ 6.00000001e-01  5.11827353e-10  6.00000001e-01 ... -7.99999999e-01\n",
      "   5.11827353e-10 -5.99999999e-01]]\n",
      "Vectorized loss and gradient: computed in 0.001037s\n",
      "[[ 8.16960000e+00 -3.55400000e+01  2.65696000e+01 ...  9.24472000e+01\n",
      "  -4.94000000e+00 -7.88496000e+01]\n",
      " [ 2.21000000e+00 -4.08200000e+01  3.04100000e+01 ...  9.45400000e+01\n",
      "  -1.96200000e+01 -1.00450000e+02]\n",
      " [-1.15524000e+01 -2.99000000e+01  4.28476000e+01 ...  1.19903200e+02\n",
      "  -4.29000000e+01 -1.24647600e+02]\n",
      " ...\n",
      " [ 4.93520000e+00 -6.08600000e+01  1.95352000e+01 ...  2.67464000e+01\n",
      "   6.54000000e+00  2.47448000e+01]\n",
      " [ 1.48000005e-02 -6.74600000e+01  1.90148000e+01 ...  5.19736000e+01\n",
      "  -4.66000000e+00  2.58652000e+01]\n",
      " [ 6.00000001e-01  5.11827353e-10  6.00000001e-01 ... -7.99999999e-01\n",
      "   5.11827353e-10 -5.99999999e-01]]\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of svm_loss_vectorized, and compute the \n",
    "# gradient of the loss function in a vectorized way.\n",
    "\n",
    "# The naive implementation and the vectorized implementation should match, \n",
    "# but the vectorized version should still be much faster.\n",
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(X_dev,y_dev,W_dev,0.000005)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "print(grad_naive)\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorized(X_dev, y_dev, \\\n",
    "                                         W_dev,0.000005)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "print(grad_vectorized)\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values \n",
    "# computed by the two implementations. The gradient on the other hand is \n",
    "# a matrix, so we use the Frobenius norm to compare them.\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('difference: %f' % difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
