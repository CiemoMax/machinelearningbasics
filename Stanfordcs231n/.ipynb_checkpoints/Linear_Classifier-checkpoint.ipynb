{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Documents/GitHub/machinelearningbasics/Stanfordcs231n/'\n",
      "/Users/xiaoxiaoma/Documents/GitHub/machinelearningbasics/Stanfordcs231n\n"
     ]
    }
   ],
   "source": [
    "# cd Documents/GitHub/machinelearningbasics/Stanfordcs231n/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear_Classifier.ipynb              \u001b[30m\u001b[43mcs231n\u001b[m\u001b[m/\r\n",
      "RunClassifiers.ipynb                 gradient_check.ipynb\r\n",
      "SVM.ipynb                            k_Nearest Neighbor Classifier.ipynb\r\n",
      "Softmax.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import import_ipynb\n",
    "import LossF_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        \n",
    "    def train(self, X, y, learning_rate = 1e-7, reg = 1e-5, \\\n",
    "              num_iters = 1500, batch_size = 200, verbose = False):\n",
    "        \"\"\"\n",
    "        Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; \n",
    "      there are N training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; \n",
    "      y[i] = c means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - reg: (float) regularization strength.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at \n",
    "      each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training\n",
    "    iteration.\n",
    "        \"\"\"\n",
    "\n",
    "        num_train, dim = X.shape\n",
    "        num_classes = np.max(y) + 1\n",
    "        if self.W is None:\n",
    "            # naive initialization\n",
    "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
    "#         print('original W: {}'.format(self.W[:5]))\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "    \n",
    "    #################################################################\n",
    "      # Sample batch_size elements from the training data and their \n",
    "      # corresponding labels to use in this round of gradient descent.\n",
    "      # Store the data in X_batch and their corresponding labels in \n",
    "      # y_batch; after sampling X_batch should have shape (dim, \n",
    "      # batch_size) and y_batch should have shape (batch_size,) \n",
    "        \n",
    "      # Hint: Use np.random.choice to generate indices. Sampling with \n",
    "      # replacement is faster than sampling without replacement.    \n",
    "      #############################################################\n",
    "    ##------------------- written by me start ---------------------##    \n",
    "\n",
    "            batch_indices = np.random.choice(num_train, batch_size)\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "  \n",
    "    ##--------------------- written by me end ---------------------##   \n",
    "    \n",
    "    # evaluate loss and gradient\n",
    "            loss, dW = self.loss(X_batch, y_batch, reg)\n",
    "            loss_history.append(loss)  \n",
    "    \n",
    "    ##############################################################\n",
    "      # Update the weights using the gradient and the learning rate.    \n",
    "      ##############################################################\n",
    "    ##------------------- written by me start ---------------------##        \n",
    "            self.W += - learning_rate * dW \n",
    "            # perform parameter update\n",
    "            \n",
    "    ##--------------------- written by me end ---------------------##          \n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "#         print('first 5 W = {}'.format(self.W[:5]))\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear classifier to predict \n",
    "    labels for data points.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; \n",
    "      there are N training samples each of dimension D.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a \n",
    "      1-dimensional array of length N, and each element is an integer \n",
    "      giving the predicted class.\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "    #################################################################\n",
    "    # TODO:                                                    \n",
    "    # Implement this method. Store the predicted labels in y_pred.      \n",
    "    #################################################################\n",
    "    \n",
    "    ##------------------- written by me start ---------------------##    \n",
    "#         print('first 5 W = {}'.format(self.W[:5]))\n",
    "        scores = X.dot(self.W)\n",
    "#         print('scores.shape = {}'.format(scores.shape))\n",
    "        y_pred = np.argmax(scores, axis = 1)\n",
    "    ##--------------------- written by me end ---------------------##  \n",
    "    \n",
    "        return y_pred\n",
    "  \n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative. \n",
    "    Subclasses will override this.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, D) containing a minibatch \n",
    "      of N data points; each point has dimension D.\n",
    "    - y_batch: A numpy array of shape (N,) containing labels for the \n",
    "      minibatch.\n",
    "    - reg: (float) regularization strength.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        pass    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return svm_loss_vectorized(X_batch, y_batch, self.W, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(LinearClassifier):\n",
    "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        return softmax_loss_vectorized(X_batch, y_batch, self.W, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "print('imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
