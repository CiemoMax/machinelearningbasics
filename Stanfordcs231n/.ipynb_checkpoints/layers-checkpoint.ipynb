{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from builtins import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
    "    # will need to reshape the input into rows.                               #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    xreshape = np.reshape(x, (x.shape[0], -1))\n",
    "    out = np.dot(xreshape, w) + b\n",
    "\n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    #### for q = f(X,W) = np.dot(X,W), y = f(q), the gradient is always: \n",
    "    #### dW = np.dot(X.T, dq)\n",
    "    #### dX = np.dot(dq, W.T)\n",
    "    dx = np.dot(dout, w.T).reshape(x.shape)\n",
    "    xreshape = np.reshape(x, (x.shape[0], -1))\n",
    "    dw = np.dot(xreshape.T, dout)\n",
    "    db = np.sum(dout, axis = 0, keepdims = True)\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    out = np.maximum(0, x)\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    mask = x > 0\n",
    "    dx = dout * mask\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are\n",
    "    computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the\n",
    "    mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "    Note that the batch normalization paper suggests a different test-time\n",
    "    behavior: they compute sample mean and variance for each feature using a\n",
    "    large number of training images rather than using a running average. For\n",
    "    this implementation we have chosen to use running averages instead since\n",
    "    they do not require an additional estimation step; the torch7\n",
    "    implementation of batch normalization also uses running averages.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    mode = bn_param['mode']\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "    out, cache = None, None\n",
    "    if mode == 'train':\n",
    "        #######################################################################\n",
    "        # TODO: Implement the training-time forward pass for batch norm.      #\n",
    "        # Use minibatch statistics to compute the mean and variance, use      #\n",
    "        # these statistics to normalize the incoming data, and scale and      #\n",
    "        # shift the normalized data using gamma and beta.                     #\n",
    "        #                                                                     #\n",
    "        # You should store the output in the variable out. Any intermediates  #\n",
    "        # that you need for the backward pass should be stored in the cache   #\n",
    "        # variable.                                                           #\n",
    "        #                                                                     #\n",
    "        # You should also use your computed sample mean and variance together #\n",
    "        # with the momentum variable to update the running mean and running   #\n",
    "        # variance, storing your result in the running_mean and running_var   #\n",
    "        # variables.                                                          #\n",
    "        #                                                                     #\n",
    "        # Note that though you should be keeping track of the running         #\n",
    "        # variance, you should normalize the data based on the standard       #\n",
    "        # deviation (square root of variance) instead!                        # \n",
    "        # Referencing the original paper (https://arxiv.org/abs/1502.03167)   #\n",
    "        # might prove to be helpful.                                          #\n",
    "        #######################################################################\n",
    "        ##------------------------ written by me start --------------------------##\n",
    "        \n",
    "#         print('input shape: ',x.shape)\n",
    "        norm = np.zeros(x.shape)\n",
    "        \n",
    "        xmean = np.mean(x, axis = 0)\n",
    "        xvar = np.var(x, axis = 0)\n",
    "        std_inv = 1/np.sqrt(xvar + eps)\n",
    "        norm = (x - xmean)*std_inv\n",
    "        \n",
    "        running_mean = \\\n",
    "              momentum * running_mean + (1 - momentum) * np.mean(x, axis = 0)\n",
    "        running_var = \\\n",
    "              momentum * running_var + (1 - momentum) * np.std(x, axis = 0)\n",
    "        \n",
    "        out = norm * gamma + beta\n",
    "        cache = (x, norm, xmean, xvar, std_inv, N, D, gamma, beta, eps)\n",
    "        \n",
    "#         print(out.dtype)\n",
    "#         print('output shape: ', out.shape)\n",
    "    \n",
    "        ##------------------------ written by me end ----------------------------##\n",
    "    elif mode == 'test':\n",
    "        #######################################################################\n",
    "        # TODO: Implement the test-time forward pass for batch normalization. #\n",
    "        # Use the running mean and variance to normalize the incoming data,   #\n",
    "        # then scale and shift the normalized data using gamma and beta.      #\n",
    "        # Store the result in the out variable.                               #\n",
    "        #######################################################################\n",
    "        ##------------------------ written by me start --------------------------##\n",
    "        norm = np.zeros(x.shape)\n",
    "        norm = (x - running_mean)/np.sqrt(running_var + eps)\n",
    "        out = norm * gamma + beta \n",
    "    \n",
    "        ##------------------------ written by me end ----------------------------##\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for batch normalization.\n",
    "\n",
    "    For this implementation, you should write out a computation graph for\n",
    "    batch normalization on paper and propagate gradients backward through\n",
    "    intermediate nodes.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from batchnorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for batch normalization. Store the    #\n",
    "    # results in the dx, dgamma, and dbeta variables.                         #\n",
    "    # Referencing the original paper (https://arxiv.org/abs/1502.03167)       #\n",
    "    # might prove to be helpful.                                              #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    # Thanks to Ref: https://wiseodd.github.io/techblog/2016/07/04/batchnorm/\n",
    "\n",
    "    #### recall:\n",
    "    #### out = (x - running_mean)/sqrt(running_var) * gamma + beta\n",
    "    #### or say:  \n",
    "    #### out = norm * gamma + beta\n",
    "    x, norm, xmean, xvar, std_inv, N, D, gamma, beta, eps = cache\n",
    "    \n",
    "    xmu = x - xmean\n",
    "    dnorm = dout * gamma\n",
    "    dvar = np.sum(dnorm * xmu, axis = 0) * -0.5 * std_inv ** 3\n",
    "    dxmu = np.sum(dnorm * -std_inv, axis = 0) + dvar * np.mean(-2. * xmu, axis = 0)\n",
    "    print('xmu shape: ', xmu.shape)\n",
    "    print('dvar shape: ', dvar.shape)\n",
    "    print('dxmu shape: ', dxmu.shape)\n",
    "    \n",
    "    dx = (dnorm * std_inv) + (dvar * 2 * xmu / N) + (dxmu /N)\n",
    "    dgamma = np.sum(dout * norm, axis = 0)\n",
    "    dbeta = np.sum(dout, axis = 0, keepdims = True)\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def batchnorm_backward_alt(dout, cache):\n",
    "    \"\"\"\n",
    "    Alternative backward pass for batch normalization.\n",
    "\n",
    "    For this implementation you should work out the derivatives for the batch\n",
    "    normalizaton backward pass on paper and simplify as much as possible. You\n",
    "    should be able to derive a simple expression for the backward pass. \n",
    "    See the jupyter notebook for more hints.\n",
    "     \n",
    "    Note: This implementation should expect to receive the same cache variable\n",
    "    as batchnorm_backward, but might not use all of the values in the cache.\n",
    "\n",
    "    Inputs / outputs: Same as batchnorm_backward\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for batch normalization. Store the    #\n",
    "    # results in the dx, dgamma, and dbeta variables.                         #\n",
    "    #                                                                         #\n",
    "    # After computing the gradient with respect to the centered inputs, you   #\n",
    "    # should be able to compute gradients with respect to the inputs in a     #\n",
    "    # single statement; our implementation fits on a single 80-character line.#\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    x, norm, xmean, xvar, std_inv, N, D, gamma, beta, eps = cache\n",
    "    \n",
    "    dx = gamma * std_inv / N * (N * dout - \\\n",
    "                       np.sum(dout * norm, axis = 0) * norm - \\\n",
    "                       np.sum(dout, axis = 0)) \n",
    "\n",
    "    dgamma = np.sum(dout * norm, axis = 0)\n",
    "    dbeta = np.sum(dout, axis = 0, keepdims = True)\n",
    "\n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layernorm_forward(x, gamma, beta, ln_param):\n",
    "    \"\"\"\n",
    "    Forward pass for layer normalization.\n",
    "\n",
    "    During both training and test-time, the incoming data is normalized per data-point,\n",
    "    before being scaled by gamma and beta parameters identical to that of batch \n",
    "    normalization.\n",
    "    \n",
    "    Note that in contrast to batch normalization, the behavior during train and test-time \n",
    "    for layer normalization are identical, and we do not need to keep track of running \n",
    "    averages of any sort.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - ln_param: Dictionary with the following keys:\n",
    "        - eps: Constant for numeric stability\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "    eps = ln_param.get('eps', 1e-5)\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the training-time forward pass for layer norm.          #\n",
    "    # Normalize the incoming data, and scale and  shift the normalized data   #\n",
    "    #  using gamma and beta.                                                  #\n",
    "    # HINT: this can be done by slightly modifying your training-time         #\n",
    "    # implementation of  batch normalization, and inserting a line or two of  #\n",
    "    # well-placed code. In particular, can you think of any matrix            #\n",
    "    # transformations you could perform, that would enable you to copy over   #\n",
    "    # the batch norm code and leave it almost unchanged?                      #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    ###### the difference between 'batchnorm' and 'layernorm' is axis = 0 vs axis = 1 \n",
    "    N, D = x.shape\n",
    "    norm = np.zeros(x.shape)\n",
    "    \n",
    "    mean_l = np.mean(x, axis = 1)\n",
    "    var_l = np.var(x, axis = 1)\n",
    "    std_inv_l = 1/np.sqrt(var_l + eps)\n",
    "    norm = ((x.T - mean_l) * std_inv_l).T\n",
    "    out = norm * gamma + beta\n",
    "    cache = (x, norm, mean_l, var_l, std_inv_l, N, D, gamma, beta, eps)\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    return out, cache\n",
    "\n",
    "def layernorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for layer normalization.\n",
    "\n",
    "    For this implementation, you can heavily rely on the work you've done already\n",
    "    for batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from layernorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for layer norm.                       #\n",
    "    #                                                                         #\n",
    "    # HINT: this can be done by slightly modifying your training-time         #\n",
    "    # implementation of batch normalization. The hints to the forward pass    #\n",
    "    # still apply!                                                            #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    x, norm, mean_l, var_l, std_inv_l, N, D, gamma, beta, eps = cache\n",
    "    \n",
    "    #### THE KEY is transpose the final dx! ####\n",
    "    dgamma = np.sum(dout * norm, axis = 0)\n",
    "    dbeta = np.sum(dout, axis = 0)\n",
    "    \n",
    "    norm = norm.T\n",
    "    dnorm = (dout * gamma).T\n",
    "    dx = 1.0/D * std_inv_l * (D * dnorm - \\\n",
    "                        np.sum(dnorm * norm, axis = 0) * norm - \\\n",
    "                        np.sum(dnorm, axis = 0))\n",
    "    dx = dx.T\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_forward(x, dropout_param):\n",
    "    \"\"\"\n",
    "    Performs the forward pass for (inverted) dropout.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of any shape\n",
    "    - dropout_param: A dictionary with the following keys:\n",
    "      - p: Dropout parameter. We keep each neuron output with probability p.\n",
    "      - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n",
    "        if the mode is test, then just return the input.\n",
    "      - seed: Seed for the random number generator. Passing seed makes this\n",
    "        function deterministic, which is needed for gradient checking but not\n",
    "        in real networks.\n",
    "\n",
    "    Outputs:\n",
    "    - out: Array of the same shape as x.\n",
    "    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout\n",
    "      mask that was used to multiply the input; in test mode, mask is None.\n",
    "\n",
    "    NOTE: Please implement **inverted** dropout, not the vanilla version of dropout.\n",
    "    See http://cs231n.github.io/neural-networks-2/#reg for more details.\n",
    "\n",
    "    NOTE 2: Keep in mind that p is the probability of **keep** a neuron\n",
    "    output; this might be contrary to some sources, where it is referred to\n",
    "    as the probability of dropping a neuron output.\n",
    "    \"\"\"\n",
    "    p, mode = dropout_param['p'], dropout_param['mode']\n",
    "    if 'seed' in dropout_param:\n",
    "        np.random.seed(dropout_param['seed'])\n",
    "\n",
    "    mask = None\n",
    "    out = None\n",
    "\n",
    "    if mode == 'train':\n",
    "        #######################################################################\n",
    "        # TODO: Implement training phase forward pass for inverted dropout.   #\n",
    "        # Store the dropout mask in the mask variable.                        #\n",
    "        #######################################################################\n",
    "        ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "        mask = (np.random.rand(*x.shape) < p ) / p\n",
    "#         print('x.head: ',x[0:5])\n",
    "#         print('mask.head: ',mask[0:5])\n",
    "        out = x * mask\n",
    "    \n",
    "        ##------------------------ written by me end ----------------------------##\n",
    "    elif mode == 'test':\n",
    "        #######################################################################\n",
    "        # TODO: Implement the test phase forward pass for inverted dropout.   #\n",
    "        #######################################################################\n",
    "        ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "        out = x\n",
    "    \n",
    "        ##------------------------ written by me end ----------------------------##\n",
    "\n",
    "    cache = (dropout_param, mask)\n",
    "    out = out.astype(x.dtype, copy=False)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Perform the backward pass for (inverted) dropout.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: (dropout_param, mask) from dropout_forward.\n",
    "    \"\"\"\n",
    "    dropout_param, mask = cache\n",
    "    mode = dropout_param['mode']\n",
    "\n",
    "    dx = None\n",
    "    if mode == 'train':\n",
    "        #######################################################################\n",
    "        # TODO: Implement training phase backward pass for inverted dropout   #\n",
    "        #######################################################################\n",
    "        ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "        dx = dout * mask\n",
    "    \n",
    "        ##------------------------ written by me end ----------------------------##\n",
    "    elif mode == 'test':\n",
    "        dx = dout\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "    The input consists of N data points, each with C channels, height H and\n",
    "    width W. We convolve each input with F different filters, where each filter\n",
    "    spans all C channels and has height HH and width WW.\n",
    "\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input. \n",
    "        \n",
    "\n",
    "    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n",
    "    along the height and width axes of the input. Be careful not to modfiy the original\n",
    "    input x directly.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional forward pass.                         #\n",
    "    # Hint: you can use the function np.pad for padding.                      #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    stride, pad = conv_param['stride'], conv_param['pad']\n",
    "    \n",
    "    if (1 + (H + 2 * pad - HH) / stride != int(1 + (H + 2 * pad - HH) / stride)) | \\\n",
    "        (1 + (W + 2 * pad - WW) / stride != int(1 + (W + 2 * pad - WW) / stride)):\n",
    "            print ('wrong setup for stride or pad')\n",
    "    \n",
    "    Hout = int(1 + (H + 2 * pad - HH) / stride)\n",
    "    Wout = int(1 + (W + 2 * pad - WW) / stride)\n",
    "    \n",
    "    xpad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "    out = np.zeros([N, F, Hout, Wout])\n",
    "\n",
    "    for n in range(N):\n",
    "        for f in range(F):\n",
    "            for hout in range(Hout):\n",
    "                for wout in range(Wout):\n",
    "                    H0 = hout * stride\n",
    "                    W0 = wout * stride\n",
    "    \n",
    "    #### it is NOT a dot product as mentioned in the slide...                  \n",
    "                    out[n,f,hout,wout] += np.sum(xpad[n,:,H0:(H0+HH),W0:(W0+WW)]*w[f]) \\\n",
    "                                          + b[f]\n",
    "        \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    cache = (x, w, b, conv_param)\n",
    "    return out, cache\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional backward pass.                        #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    x, w, b, conv_param = cache\n",
    "    N, C, H, W = x.shape\n",
    "    F, _, HH, WW = w.shape\n",
    "    _, _, Hout, Wout = dout.shape\n",
    "    stride, pad = conv_param['stride'], conv_param['pad']\n",
    "    xpad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "    dxpad = np.zeros(xpad.shape)\n",
    "    dw = np.zeros(w.shape)\n",
    "    db = np.zeros(b.shape)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for f in range(F):\n",
    "            db[f] += np.sum(dout[n,f])\n",
    "            for hout in range(Hout):\n",
    "                for wout in range(Wout):\n",
    "                    H0 = hout * stride\n",
    "                    W0 = wout * stride\n",
    "                    dw[f] += xpad[n,:,H0:(H0+HH), W0:(W0+WW)] * dout[n,f,hout,wout]\n",
    "                    dxpad[n,:,H0:(H0+HH), W0:(W0+WW)] += w[f] * dout[n,f,hout,wout]\n",
    "    \n",
    "    dx = dxpad[:,:,pad:-pad, pad:-pad]\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_forward_naive(x, pool_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a max-pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C, H, W)\n",
    "    - pool_param: dictionary with the following keys:\n",
    "      - 'pool_height': The height of each pooling region\n",
    "      - 'pool_width': The width of each pooling region\n",
    "      - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "    No padding is necessary here. Output size is given by \n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H - pool_height) / stride\n",
    "      W' = 1 + (W - pool_width) / stride\n",
    "    - cache: (x, pool_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the max-pooling forward pass                            #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    N, C, H, W = x.shape\n",
    "    pool_H, pool_W, stride = pool_param['pool_height'], \\\n",
    "                            pool_param['pool_width'],\\\n",
    "                            pool_param['stride']\n",
    "    Hout = int(1 + (H - pool_H) / stride)\n",
    "    Wout = int(1 + (W - pool_W) / stride)\n",
    "    \n",
    "    if (1 + (H - pool_H) / stride != Hout) | (1 + (W - pool_W) / stride != Wout):\n",
    "            print ('wrong pooling size or stride')\n",
    "    \n",
    "    out = np.zeros([N, C, Hout, Wout])\n",
    "    \n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for hout in range(Hout):\n",
    "                for wout in range(Wout):\n",
    "                    H0 = hout * stride\n",
    "                    W0 = wout * stride\n",
    "                    out[n,c,hout,wout] = np.max(x[n,c,H0:(H0+pool_H),W0:(W0+pool_W)])\n",
    "    \n",
    "    cache = x, pool_param\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    cache = (x, pool_param)\n",
    "    return out, cache\n",
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a max-pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives\n",
    "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the max-pooling backward pass                           #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    x, pool_param = cache\n",
    "    N, C, H, W = x.shape\n",
    "    pool_H, pool_W, stride = pool_param['pool_height'], \\\n",
    "                            pool_param['pool_width'],\\\n",
    "                            pool_param['stride']\n",
    "    _, _, Hout,Wout = dout.shape\n",
    "    \n",
    "    dx = np.zeros(x.shape)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for hout in range(Hout):\n",
    "                for wout in range(Wout):\n",
    "                    H0 = hout * stride\n",
    "                    W0 = wout * stride\n",
    "                    dx_mask = np.unravel_index(np.argmax(x[n, c, \\\n",
    "                                                           H0:(H0 + pool_H),\\\n",
    "                                                           W0:(W0 + pool_W)], \\\n",
    "                                                         axis=None), (pool_H, pool_W))\n",
    "                    dx[n, c, H0:(H0 + pool_H),W0:(W0 + pool_W)][dx_mask] = \\\n",
    "                                                                    dout[n, c, hout, wout]\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - gamma: Scale parameter, of shape (C,)\n",
    "    - beta: Shift parameter, of shape (C,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance. momentum=0 means that\n",
    "        old information is discarded completely at every time step, while\n",
    "        momentum=1 means that new information is never incorporated. The\n",
    "        default of momentum=0.9 should work well in most situations.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H, W)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the forward pass for spatial batch normalization.       #\n",
    "    #                                                                         #\n",
    "    # HINT: You can implement spatial batch normalization by calling the      #\n",
    "    # vanilla version of batch normalization you implemented above.           #\n",
    "    # Your implementation should be very short; ours is less than five lines. #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "#     Ref: https://github.com/haofeixu/standford-cs231n-2018/blob/master/assignment2/cs231n/layers.py\n",
    "    N, C, H, W = x.shape\n",
    "    \n",
    "    x_sbm = np.reshape(np.transpose(x, (0, 2, 3, 1)), (-1, C))\n",
    "    \n",
    "    out, cache = batchnorm_forward(x_sbm, gamma, beta, bn_param)\n",
    "    \n",
    "    out = np.transpose(np.reshape(out, (N, H, W, C)), (0, 3, 1, 2))\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def spatial_batchnorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "    - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "    - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for spatial batch normalization.      #\n",
    "    #                                                                         #\n",
    "    # HINT: You can implement spatial batch normalization by calling the      #\n",
    "    # vanilla version of batch normalization you implemented above.           #\n",
    "    # Your implementation should be very short; ours is less than five lines. #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    N, C, H, W = dout.shape\n",
    "    \n",
    "    dout_sbm = np.reshape(np.transpose(dout, (0,2,3,1)), (-1, C))\n",
    "    \n",
    "    dx, dgamma, dbeta = batchnorm_backward_alt(dout_sbm, cache)\n",
    "    \n",
    "    dx = np.transpose(np.reshape(dx, (N, H, W, C)), (0,3,1,2))\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_groupnorm_forward(x, gamma, beta, G, gn_param):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for spatial group normalization.\n",
    "    In contrast to layer normalization, group normalization splits each entry \n",
    "    in the data into G contiguous pieces, which it then normalizes independently.\n",
    "    Per feature shifting and scaling are then applied to the data, in a manner identical \n",
    "    to that of batch normalization and layer normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - gamma: Scale parameter, of shape (C,)\n",
    "    - beta: Shift parameter, of shape (C,)\n",
    "    - G: Integer mumber of groups to split into, should be a divisor of C\n",
    "    - gn_param: Dictionary with the following keys:\n",
    "      - eps: Constant for numeric stability\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H, W)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "    eps = gn_param.get('eps',1e-5)\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the forward pass for spatial group normalization.       #\n",
    "    # This will be extremely similar to the layer norm implementation.        #\n",
    "    # In particular, think about how you could transform the matrix so that   #\n",
    "    # the bulk of the code is similar to both train-time batch normalization  #\n",
    "    # and layer normalization!                                                # \n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    N, C, H, W = x.shape\n",
    "    x = np.reshape(x, (N * G, (C//G) * H * W))\n",
    "    \n",
    "    mean_sg = np.mean(x, axis = 1)\n",
    "    var_sg= np.var(x, axis = 1)\n",
    "    std_inv_sg = 1/np.sqrt(var_sg + eps)\n",
    "    norm = ((x.T - mean_sg) * std_inv_sg).T\n",
    "    \n",
    "    norm = np.reshape(norm, (N, C, H, W))\n",
    "#     Ref: https://github.com/haofeixu/standford-cs231n-2018/blob/master/assignment2/cs231n/layers.py\n",
    "    out = gamma[np.newaxis, :, np.newaxis, np.newaxis] * norm + \\\n",
    "           beta[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "    cache = (x, norm, mean_sg, var_sg, std_inv_sg, G, gamma, beta, eps)\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    return out, cache\n",
    "\n",
    "def spatial_groupnorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for spatial group normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "    - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "    - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for spatial group normalization.      #\n",
    "    # This will be extremely similar to the layer norm implementation.        #\n",
    "    ###########################################################################\n",
    "    ##------------------------ written by me start --------------------------##\n",
    "    \n",
    "    x, norm, mean_sg, var_sg, std_inv_sg, G, gamma, beta, eps = cache\n",
    "    N, C, H, W = x.shape\n",
    "    \n",
    "    dgamma = np.sum(dout * norm, axis = (0,2,3), keepdims = True)\n",
    "    dbeta = np.sum(dout, axis = (0,2,3), keepdims = True)\n",
    "    \n",
    "    dnorm = (dout * gamma[np.newaxis, :, np.newaxis, \\\n",
    "                         np.newaxis]).reshape(dnorm,(N*G, (C//G)*H*W)).T\n",
    "    norm = np.reshape(norm, (N*G, (C//G)*H*W)).T\n",
    "    \n",
    "    D = (C//G)*H*W\n",
    "    dx = 1/D * std_inv * ((D * dnorm - \\\n",
    "                           np.sum(dnorm, axis = 0)) -\\\n",
    "                           np.sum(dnorm * norm, axis = 0) * norm)\n",
    "    dx = np.reshape(dx.T, (N,C,H,W))\n",
    "    \n",
    "    ##------------------------ written by me end ----------------------------##\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient using for multiclass SVM classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    correct_class_scores = x[np.arange(N), y]\n",
    "    margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n",
    "    margins[np.arange(N), y] = 0\n",
    "    loss = np.sum(margins) / N\n",
    "    num_pos = np.sum(margins > 0, axis=1)\n",
    "    dx = np.zeros_like(x)\n",
    "    dx[margins > 0] = 1\n",
    "    dx[np.arange(N), y] -= num_pos\n",
    "    dx /= N\n",
    "    return loss, dx\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "    log_probs = shifted_logits - np.log(Z)\n",
    "    probs = np.exp(log_probs)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
